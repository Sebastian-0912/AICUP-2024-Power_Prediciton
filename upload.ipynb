{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformer import TransformerModel\n",
    "from utils.mask import generate_tgt_mask\n",
    "\n",
    "def prepare_prediction_data(input_str, base_path, scaler):\n",
    "    \"\"\"\n",
    "    Prepare data for prediction based on input string.\n",
    "    \n",
    "    Args:\n",
    "        input_str (str): The input string specifying the prediction target (e.g., \"20240117090001\").\n",
    "        base_path (str): Path to the folder containing location-specific CSV files.\n",
    "        scaler (MinMaxScaler): Scaler used to transform features.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Prepared input tensor for the model.\n",
    "    \"\"\"\n",
    "    year, month, day, hour, minute, location = (\n",
    "        input_str[:4],\n",
    "        input_str[4:6],\n",
    "        input_str[6:8],\n",
    "        input_str[8:10],\n",
    "        input_str[10:12],\n",
    "        f\"L{input_str[12:13]}\"\n",
    "    )\n",
    "    target_datetime = datetime.strptime(f\"{year}-{month}-{day} {hour}:{minute}\", \"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    # Define the time ranges for the source data\n",
    "    start_prev_day = target_datetime - timedelta(days=1, hours=target_datetime.hour - 7)\n",
    "    end_prev_day = target_datetime - timedelta(days=1, hours=target_datetime.hour - 17)\n",
    "    start_today = target_datetime - timedelta(hours=target_datetime.hour - 7)\n",
    "    end_today = target_datetime - timedelta(minutes=target_datetime.minute)\n",
    "\n",
    "    # Load data for the location\n",
    "    data_path = os.path.join(base_path, f\"{location}_Train_resampled.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "\n",
    "    # Filter the required data\n",
    "    prev_day_data = df[(df['DateTime'] >= start_prev_day) & (df['DateTime'] <= end_prev_day)]\n",
    "    today_data = df[(df['DateTime'] >= start_today) & (df['DateTime'] <= end_today)]\n",
    "\n",
    "    # Combine data and extract features\n",
    "    combined_data = pd.concat([prev_day_data, today_data])\n",
    "    features = combined_data.drop(columns=[\"DateTime\", \"Power(mW)\"]).values\n",
    "    features = scaler.transform(features)  # Apply Min-Max scaling\n",
    "\n",
    "    # Convert to tensor\n",
    "    return torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-17 09:00:00\n",
      "1\n",
      "2024-01-19 09:00:00\n",
      "1\n",
      "2024-02-24 09:00:00\n",
      "1\n",
      "2024-02-26 09:00:00\n",
      "1\n",
      "2024-03-27 09:00:00\n",
      "1\n",
      "2024-03-29 09:00:00\n",
      "1\n",
      "2024-04-28 09:00:00\n",
      "1\n",
      "2024-04-30 09:00:00\n",
      "1\n",
      "2024-05-15 09:00:00\n",
      "1\n",
      "2024-05-20 09:00:00\n",
      "1\n",
      "2024-06-13 09:00:00\n",
      "1\n",
      "2024-06-15 09:00:00\n",
      "1\n",
      "2024-07-03 09:00:00\n",
      "1\n",
      "2024-07-07 09:00:00\n",
      "1\n",
      "2024-01-25 09:00:00\n",
      "2\n",
      "2024-01-28 09:00:00\n",
      "2\n",
      "2024-02-13 09:00:00\n",
      "2\n",
      "2024-02-26 09:00:00\n",
      "2\n",
      "2024-03-19 09:00:00\n",
      "2\n",
      "2024-04-27 09:00:00\n",
      "2\n",
      "2024-05-08 09:00:00\n",
      "2\n",
      "2024-07-05 09:00:00\n",
      "2\n",
      "2024-07-07 09:00:00\n",
      "2\n",
      "2024-03-17 09:00:00\n",
      "3\n",
      "2024-04-14 09:00:00\n",
      "3\n",
      "2024-05-24 09:00:00\n",
      "3\n",
      "2024-05-26 09:00:00\n",
      "3\n",
      "2024-06-03 09:00:00\n",
      "3\n",
      "2024-06-20 09:00:00\n",
      "3\n",
      "2024-04-20 09:00:00\n",
      "4\n",
      "2024-04-22 09:00:00\n",
      "4\n",
      "2024-05-13 09:00:00\n",
      "4\n",
      "2024-05-15 09:00:00\n",
      "4\n",
      "2024-06-29 09:00:00\n",
      "4\n",
      "2024-07-04 09:00:00\n",
      "4\n",
      "2024-05-09 09:00:00\n",
      "5\n",
      "2024-05-14 09:00:00\n",
      "5\n",
      "2024-06-15 09:00:00\n",
      "5\n",
      "2024-06-20 09:00:00\n",
      "5\n",
      "2024-07-06 09:00:00\n",
      "5\n",
      "2024-07-08 09:00:00\n",
      "5\n",
      "2024-05-20 09:00:00\n",
      "6\n",
      "2024-05-27 09:00:00\n",
      "6\n",
      "2024-06-25 09:00:00\n",
      "6\n",
      "2024-06-29 09:00:00\n",
      "6\n",
      "2024-07-06 09:00:00\n",
      "6\n",
      "2024-07-22 09:00:00\n",
      "6\n",
      "2024-05-26 09:00:00\n",
      "7\n",
      "2024-06-12 09:00:00\n",
      "7\n",
      "2024-06-15 09:00:00\n",
      "7\n",
      "2024-07-17 09:00:00\n",
      "7\n",
      "2024-01-24 09:00:00\n",
      "8\n",
      "2024-01-28 09:00:00\n",
      "8\n",
      "2024-02-04 09:00:00\n",
      "8\n",
      "2024-02-18 09:00:00\n",
      "8\n",
      "2024-03-05 09:00:00\n",
      "8\n",
      "2024-03-25 09:00:00\n",
      "8\n",
      "2024-05-18 09:00:00\n",
      "8\n",
      "2024-05-26 09:00:00\n",
      "8\n",
      "2024-05-29 09:00:00\n",
      "8\n",
      "2024-06-07 09:00:00\n",
      "8\n",
      "2024-06-13 09:00:00\n",
      "8\n",
      "2024-06-28 09:00:00\n",
      "8\n",
      "2024-07-05 09:00:00\n",
      "8\n",
      "2024-07-09 09:00:00\n",
      "8\n",
      "2024-07-15 09:00:00\n",
      "8\n",
      "2024-07-19 09:00:00\n",
      "8\n",
      "2024-03-09 09:00:00\n",
      "9\n",
      "2024-03-14 09:00:00\n",
      "9\n",
      "2024-04-14 09:00:00\n",
      "9\n",
      "2024-05-06 09:00:00\n",
      "9\n",
      "2024-05-26 09:00:00\n",
      "9\n",
      "2024-06-14 09:00:00\n",
      "9\n",
      "2024-06-21 09:00:00\n",
      "9\n",
      "2024-06-28 09:00:00\n",
      "9\n",
      "2024-07-17 09:00:00\n",
      "9\n",
      "2024-03-16 09:00:00\n",
      "10\n",
      "2024-03-24 09:00:00\n",
      "10\n",
      "2024-04-12 09:00:00\n",
      "10\n",
      "2024-04-19 09:00:00\n",
      "10\n",
      "2024-05-15 09:00:00\n",
      "10\n",
      "2024-05-26 09:00:00\n",
      "10\n",
      "2024-06-17 09:00:00\n",
      "10\n",
      "2024-06-30 09:00:00\n",
      "10\n",
      "2024-07-14 09:00:00\n",
      "10\n",
      "2024-07-18 09:00:00\n",
      "10\n",
      "2024-07-20 09:00:00\n",
      "10\n",
      "2024-03-22 09:00:00\n",
      "11\n",
      "2024-03-27 09:00:00\n",
      "11\n",
      "2024-04-13 09:00:00\n",
      "11\n",
      "2024-04-26 09:00:00\n",
      "11\n",
      "2024-06-06 09:00:00\n",
      "11\n",
      "2024-07-12 09:00:00\n",
      "11\n",
      "2024-03-15 09:00:00\n",
      "12\n",
      "2024-03-17 09:00:00\n",
      "12\n",
      "2024-04-12 09:00:00\n",
      "12\n",
      "2024-04-26 09:00:00\n",
      "12\n",
      "2024-05-21 09:00:00\n",
      "12\n",
      "2024-06-19 09:00:00\n",
      "12\n",
      "2024-06-21 09:00:00\n",
      "12\n",
      "2024-07-05 09:00:00\n",
      "12\n",
      "2024-07-13 09:00:00\n",
      "12\n",
      "2024-02-14 09:00:00\n",
      "13\n",
      "2024-02-21 09:00:00\n",
      "13\n",
      "2024-03-14 09:00:00\n",
      "13\n",
      "2024-03-16 09:00:00\n",
      "13\n",
      "2024-03-28 09:00:00\n",
      "13\n",
      "2024-04-12 09:00:00\n",
      "13\n",
      "2024-06-19 09:00:00\n",
      "13\n",
      "2024-02-25 09:00:00\n",
      "14\n",
      "2024-02-27 09:00:00\n",
      "14\n",
      "2024-03-27 09:00:00\n",
      "14\n",
      "2024-04-17 09:00:00\n",
      "14\n",
      "2024-05-17 09:00:00\n",
      "14\n",
      "2024-07-04 09:00:00\n",
      "14\n",
      "2024-07-15 09:00:00\n",
      "14\n",
      "2024-02-15 09:00:00\n",
      "15\n",
      "2024-02-18 09:00:00\n",
      "15\n",
      "2024-03-14 09:00:00\n",
      "15\n",
      "2024-03-17 09:00:00\n",
      "15\n",
      "2024-04-12 09:00:00\n",
      "15\n",
      "2024-05-18 09:00:00\n",
      "15\n",
      "2024-05-30 09:00:00\n",
      "15\n",
      "2024-06-05 09:00:00\n",
      "15\n",
      "2024-07-04 09:00:00\n",
      "15\n",
      "2024-03-16 09:00:00\n",
      "16\n",
      "2024-03-22 09:00:00\n",
      "16\n",
      "2024-04-14 09:00:00\n",
      "16\n",
      "2024-04-22 09:00:00\n",
      "16\n",
      "2024-05-13 09:00:00\n",
      "16\n",
      "2024-05-17 09:00:00\n",
      "16\n",
      "2024-05-19 09:00:00\n",
      "16\n",
      "2024-06-08 09:00:00\n",
      "16\n",
      "2024-06-29 09:00:00\n",
      "16\n",
      "2024-01-23 09:00:00\n",
      "17\n",
      "2024-01-29 09:00:00\n",
      "17\n",
      "2024-02-15 09:00:00\n",
      "17\n",
      "2024-02-18 09:00:00\n",
      "17\n",
      "2024-03-05 09:00:00\n",
      "17\n",
      "2024-03-09 09:00:00\n",
      "17\n",
      "2024-03-11 09:00:00\n",
      "17\n",
      "2024-04-03 09:00:00\n",
      "17\n",
      "2024-05-07 09:00:00\n",
      "17\n",
      "2024-05-28 09:00:00\n",
      "17\n",
      "2024-06-14 09:00:00\n",
      "17\n",
      "2024-06-19 09:00:00\n",
      "17\n",
      "2024-06-27 09:00:00\n",
      "17\n",
      "2024-06-29 09:00:00\n",
      "17\n",
      "2024-07-04 09:00:00\n",
      "17\n",
      "2024-07-06 09:00:00\n",
      "17\n",
      "2024-03-28 09:00:00\n",
      "8\n",
      "2024-05-08 09:00:00\n",
      "8\n",
      "2024-05-21 09:00:00\n",
      "8\n",
      "2024-06-18 09:00:00\n",
      "8\n",
      "2024-06-30 09:00:00\n",
      "8\n",
      "2024-07-02 09:00:00\n",
      "8\n",
      "2024-08-12 09:00:00\n",
      "8\n",
      "2024-08-28 09:00:00\n",
      "8\n",
      "2024-03-07 09:00:00\n",
      "10\n",
      "2024-03-10 09:00:00\n",
      "10\n",
      "2024-04-23 09:00:00\n",
      "10\n",
      "2024-05-19 09:00:00\n",
      "10\n",
      "2024-06-19 09:00:00\n",
      "10\n",
      "2024-07-11 09:00:00\n",
      "10\n",
      "2024-02-07 09:00:00\n",
      "17\n",
      "2024-02-24 09:00:00\n",
      "17\n",
      "2024-03-18 09:00:00\n",
      "17\n",
      "2024-05-09 09:00:00\n",
      "17\n",
      "2024-06-02 09:00:00\n",
      "17\n",
      "2024-06-04 09:00:00\n",
      "17\n",
      "2024-06-09 09:00:00\n",
      "17\n",
      "2024-07-09 09:00:00\n",
      "17\n",
      "2024-01-06 09:00:00\n",
      "1\n",
      "2024-01-11 09:00:00\n",
      "1\n",
      "2024-02-13 09:00:00\n",
      "1\n",
      "2024-05-08 09:00:00\n",
      "1\n",
      "2024-05-10 09:00:00\n",
      "1\n",
      "2024-05-24 09:00:00\n",
      "1\n",
      "2024-06-23 09:00:00\n",
      "1\n",
      "2024-06-27 09:00:00\n",
      "1\n",
      "2024-07-13 09:00:00\n",
      "1\n",
      "2024-07-15 09:00:00\n",
      "1\n",
      "2024-09-27 09:00:00\n",
      "4\n",
      "2024-10-18 09:00:00\n",
      "4\n",
      "2024-09-18 09:00:00\n",
      "7\n",
      "2024-09-20 09:00:00\n",
      "7\n",
      "2024-10-16 09:00:00\n",
      "7\n",
      "2024-09-22 09:00:00\n",
      "8\n",
      "2024-09-28 09:00:00\n",
      "8\n",
      "2024-10-11 09:00:00\n",
      "8\n",
      "2024-10-16 09:00:00\n",
      "8\n",
      "2024-10-18 09:00:00\n",
      "8\n",
      "2024-10-28 09:00:00\n",
      "8\n",
      "2024-09-22 09:00:00\n",
      "9\n",
      "2024-09-28 09:00:00\n",
      "9\n",
      "2024-09-20 09:00:00\n",
      "10\n",
      "2024-09-28 09:00:00\n",
      "10\n",
      "2024-09-19 09:00:00\n",
      "12\n",
      "2024-09-23 09:00:00\n",
      "12\n",
      "2024-10-02 09:00:00\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformer import TransformerModel\n",
    "from utils.mask import generate_tgt_mask\n",
    "\n",
    "def predict_single_data(model: TransformerModel, data:pd.DataFrame):\n",
    "  \n",
    "  return\n",
    "  \n",
    "def process_upload_data(upload_path):\n",
    "  data = pd.read_csv(upload_path)\n",
    "  data['DateTime'] = data['序號'].apply(lambda x: str(x)[:12])\n",
    "  data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "  data['loactoin'] = data['序號'].apply(lambda x: str(x)[13:] if str(x)[12]== '0' else str(x)[12:] )\n",
    "  return data\n",
    "\n",
    "def main():\n",
    "  upload_data = process_upload_data(\"dataset/upload.csv\")\n",
    "  for i in range(0, len(upload_data), 48):\n",
    "    predicted_date, location_id = upload_data.iloc[i][['DateTime', 'loactoin']]\n",
    "    location_data =pd.read_csv(f\"./dataset/36_TrainingData_interpolation_process/L{location_id}_Train_resampled.csv\")\n",
    "    location_data.between_time() #find the data in predicted_date - 1day to predicted_date\n",
    "    \n",
    "    predict_single_data() # \n",
    "    \n",
    "main()\n",
    "\n",
    "# process_upload_data(\"dataset/upload.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2292517/2422745134.py:101: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "/home/sebastian/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to upload_with_predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[5], line 108\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mscaler\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Perform prediction\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     predicted_power \u001b[38;5;241m=\u001b[39m predict_single_data(model, input_data, scaler)\n\u001b[1;32m    109\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(predicted_power)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Generate upload file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mpredict_single_data\u001b[0;34m(model, data, scaler)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Preprocess the input data\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPower(mW)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPower(mW)\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Normalize using the same scaler as training\u001b[39;00m\n\u001b[1;32m     30\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (1, 72, features)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Prepare input for the decoder (12 known timesteps from the second day)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:534\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    530\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    532\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 534\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    535\u001b[0m     X,\n\u001b[1;32m    536\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[1;32m    537\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[1;32m    538\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    539\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    540\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    543\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[1;32m    544\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m             )\n\u001b[0;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from dataloader import SolarPowerDataset\n",
    "from transformer import TransformerModel\n",
    "from utils.mask import generate_tgt_mask\n",
    "\n",
    "\n",
    "def predict_single_data(model: TransformerModel, data: pd.DataFrame, scaler: MinMaxScaler):\n",
    "    \"\"\"\n",
    "    Predict solar power for a single input dataset.\n",
    "\n",
    "    Args:\n",
    "        model (TransformerModel): Trained model for prediction.\n",
    "        data (pd.DataFrame): Input data for prediction (72 timesteps).\n",
    "        scaler (MinMaxScaler): Scaler used for normalizing data during training.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Predicted power for 48 timesteps.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(type(data))\n",
    "    # Preprocess the input data\n",
    "    data['Power(mW)'] = scaler.transform(data['Power(mW)'])  # Normalize using the same scaler as training\n",
    "    input_data = torch.tensor(data, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 72, features)\n",
    "\n",
    "    # Prepare input for the decoder (12 known timesteps from the second day)\n",
    "    tgt_input = input_data[:, -12:, :]  # (1, 12, features)\n",
    "\n",
    "    # Perform autoregressive prediction\n",
    "    with torch.no_grad():\n",
    "        for _ in range(48):\n",
    "            tgt_mask = generate_tgt_mask(tgt_input.size(1)).to(device)  # Update mask\n",
    "            output = model(input_data, tgt_input, tgt_mask=tgt_mask)  # Model prediction\n",
    "            next_step = output[:, -1:, :]  # Get the last timestep prediction\n",
    "            tgt_input = torch.cat([tgt_input, next_step], dim=1)  # Append the prediction\n",
    "\n",
    "    # Extract the predicted values and inverse transform\n",
    "    predicted_power = tgt_input[:, 12:, :].squeeze(0).cpu().numpy()  # (48, features)\n",
    "    predicted_power = scaler.inverse_transform(predicted_power)[:, 0]  # Extract power (first feature)\n",
    "    return predicted_power\n",
    "\n",
    "\n",
    "def process_upload_data(upload_path):\n",
    "    \"\"\"\n",
    "    Process the upload.csv file into a structured DataFrame.\n",
    "\n",
    "    Args:\n",
    "        upload_path (str): Path to the upload.csv file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Structured DataFrame with parsed DateTime and location.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(upload_path)\n",
    "    data['DateTime'] = data['序號'].apply(lambda x: str(x)[:12])\n",
    "    data['DateTime'] = pd.to_datetime(data['DateTime'], format='%Y%m%d%H%M')\n",
    "    data['location'] = data['序號'].apply(lambda x: str(x)[13:] if str(x)[12] == '0' else str(x)[12:])\n",
    "    return data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process the public test dataset and generate predictions.\n",
    "    \"\"\"\n",
    "    # Load the upload data\n",
    "    upload_data = process_upload_data(\"dataset/upload.csv\")\n",
    "\n",
    "    # Initialize the prediction output\n",
    "    predictions = []\n",
    "\n",
    "    # Loop through each unique location and date pair\n",
    "    for i in range(0, len(upload_data), 48):\n",
    "        predicted_date, location_id = upload_data.iloc[i][['DateTime', 'location']]\n",
    "        \n",
    "        # Load location-specific training data\n",
    "        location_data = pd.read_csv(f\"./dataset/36_TrainingData_interpolation_process/L{location_id}_Train_resampled.csv\")\n",
    "        location_data['DateTime'] = pd.to_datetime(location_data['DateTime'])\n",
    "\n",
    "        # Extract required input data for the model\n",
    "        start_time = predicted_date - timedelta(days=1, hours=2, minutes=0)\n",
    "        end_time = predicted_date + timedelta(hours=7, minutes=50)\n",
    "        input_data = location_data.set_index('DateTime').loc[start_time:end_time]\n",
    "        \n",
    "        # Initialize and load the trained model\n",
    "        model_path = f\"./model_pth/v3/location_{location_id}/L_{location_id}_ep_2000.pth\"\n",
    "        model = TransformerModel(\n",
    "            src_input_dim=12,\n",
    "            tgt_input_dim=1,\n",
    "            d_model=128,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=5,\n",
    "            num_decoder_layers=5,\n",
    "            dim_feedforward=128,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        # Load the scaler used during training\n",
    "        dataset = SolarPowerDataset(data_path=f\"./dataset/36_TrainingData_interpolation_process/L{location_id}_Train_resampled.csv\")\n",
    "        scaler = dataset.scaler\n",
    "\n",
    "        # Perform prediction\n",
    "        predicted_power = predict_single_data(model, input_data, scaler)\n",
    "        predictions.append(predicted_power)\n",
    "\n",
    "    # Generate upload file\n",
    "    flat_predictions = [val for sublist in predictions for val in sublist]\n",
    "    upload_data['答案'] = flat_predictions\n",
    "    upload_data.to_csv(\"upload_with_predictions.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Predictions saved to upload_with_predictions.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "up = pd.read_csv('upload_with_predictions.csv')\n",
    "up = up.drop(columns=['DateTime', 'location'])\n",
    "up.to_csv('./v3_upload.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
